# LeadsProject

A fully modular, end-to-end pipeline for scraping, enriching, summarizing, and storing "lead" profiles (people) into a PostgreSQL + pgvector database, designed to run locally with an NVIDIA RTXÂ 4090 and a dedicated NVMe drive.

---

## ğŸ“‚ Directory Structure

```
~/leads
â”œâ”€â”€ Dockerfile                    # Container definition for the orchestrator
â”œâ”€â”€ docker-compose.yml            # Compose file for PostgreSQL + (optionally) the app
â”œâ”€â”€ envs
â”‚   â””â”€â”€ scraper_env.yml           # Conda environment spec for scraper dependencies
â”œâ”€â”€ models                        # All task-specific model folders (via manifest)
â”‚   â”œâ”€â”€ manifest.json             # JSON listing all models to download
â”‚   â”œâ”€â”€ ner_model/                # NER model (Davlan/bertâ€¦)
â”‚   â”œâ”€â”€ summarizer_model/         # Summarization model (e.g. OpenHermes 7B)
â”‚   â”œâ”€â”€ bio_generator_model/      # Bio/expertise generation model (e.g. Llama2-chat)
â”‚   â”œâ”€â”€ quality_scorer/           # Writing quality classifier (DeBERTa)
â”‚   â””â”€â”€ embedder_model/           # Embeddings (E5-large-v2)
â”œâ”€â”€ db
â”‚   â”œâ”€â”€ init.sql                  # DDL: create tables & extensions
â”‚   â”œâ”€â”€ seed.sql                  # Optional starter data
â”‚   â”œâ”€â”€ cfg.py                    # SQLAlchemy connection config
â”‚   â””â”€â”€ models.py                 # SQLAlchemy ORM definitions
â”œâ”€â”€ scr                           # Scraper code & orchestrator
â”‚   â”œâ”€â”€ orch.py                   # Flask API / orchestrator entrypoint
â”‚   â”œâ”€â”€ config.py                 # API keys & defaults (dotenv)
â”‚   â”œâ”€â”€ phases                    # Four sequential pipeline modules
â”‚   â”‚   â”œâ”€â”€ p1_initial.py         # PhaseÂ 1: Google CSE + NER extraction
â”‚   â”‚   â”œâ”€â”€ p2_tangential.py      # PhaseÂ 2: drillâ€down search & email extraction
â”‚   â”‚   â”œâ”€â”€ p3_summarizer.py      # PhaseÂ 3: summarization (Mistralâ€7B or stub)
â”‚   â”‚   â””â”€â”€ p4_sorter.py          # PhaseÂ 4: normalize fields & call DB insert
â”‚   â””â”€â”€ utils                     # Shared utilities
â”‚       â”œâ”€â”€ ner.py                # NER extraction pipeline
â”‚       â”œâ”€â”€ email_extractor.py    # Regex & obfuscated email parsing
â”‚       â”œâ”€â”€ fetcher.py            # Async HTTP + Playwright fetchers
â”‚       â”œâ”€â”€ google_cse.py         # Google Custom Search JSON API wrapper
â”‚       â”œâ”€â”€ html_parser.py        # BeautifulSoup-based HTMLâ†’text
â”‚       â””â”€â”€ download_models.py    # Manifest-driven model downloader
â””â”€â”€ README.md                     # This documentation
```

---

## ğŸ”„ Architecture Overview

1. **Orchestration (`scr/orch.py`)**

   * Provides a Flask endpoint (`POST /run`) or CLI entrypoint
   * Invokes four phases sequentially (async where appropriate)
   * After PhaseÂ 4, pushes data into PostgreSQL via SQLAlchemy helpers

2. **PhaseÂ 1: Initial Search & NER**

   * Query Google Custom Search (CX JSON API) for `industry + location` or free-text prompts
   * Fetch each landing page concurrently
   * Run Hugging Face `Davlan/bert-base-multilingual-cased-ner-hrl` for PERSON & ORG entities
   * Collect names, contexts, and URLs for drill-down

3. **PhaseÂ 2: Tangential Searches & Email Extraction**

   * For each candidate name: query Google CSE for `"Name" email` and `"Name" profile`
   * If the URL domain requires JS (LinkedIn, RocketReach), use Playwright; else use aiohttp
   * Parse page text â†’ extract emails (raw + obfuscated) and context snippets

4. **PhaseÂ 3: Summarizer**

   * Summarize cleaned HTML/text using Mistral-7B (`models/summarizer_model`) via 4-bit quantization
   * Generate a concise summary relevant to the original prompt
   * (During testing, you can stub this step to use the first snippet)

5. **PhaseÂ 4: Sorter & DB Insertion**

   * Normalize each profile into standard fields: `name`, `summary`, `bio`, `emails`, `affiliations`, `sources`, `quality` score, `embedding`
   * Compute embeddings via E5-large-v2 for vector similarity searches
   * Score quality using writing-quality classifier (DeBERTa) or heuristic formula
   * Call `db/db_ops.insert_lead()` to insert into PostgreSQL tables

6. **Database (PostgreSQLÂ +Â pgvector)**

   * Data directory: `/data/ldb/pgdata` (bind-mounted to `/data/bookproject/ldb/pgdata` on the Kingston NVMe)
   * Tables:

     * `person` (intÂ PK, full\_name, first/last, title, location, bio, summary, quality\_score, embedding)
     * `email` (intÂ PK, person\_id FK, address, confidence, is\_primary)
     * `organization` (intÂ PK, name, org\_type, country)
     * `affiliation` (person\_id, org\_id, role)
     * `source` (person\_id, url, snippet, source\_rank, fetched\_at)
     * `edge` (from\_id, to\_id, rel\_type, weight)
   * Extensions: `vector` (pgvector), `pg_trgm` (fuzzy search)

---

## âš™ï¸ Getting Started

1. **Clone & scaffold**

   ```bash
   cd ~
   git clone <your-repo-url> leads
   cd leads
   ./scr/utils/download_models.py   # download all models via manifest.json
   ```

2. **Create Conda env**

   ```bash
   conda env create -f envs/scraper_env.yml
   conda activate scraper
   ```

3. **Start the database**

   ```bash
   # ensure /data/bookproject/ldb/pgdata is bind-mounted to /data/ldb
   docker compose up -d db
   docker logs -f leads_postgres
   # confirm tables created
   psql -h localhost -U leads_user -d leadsdb -c '\dt'
   ```

4. **Run the pipeline**

   ```bash
   # CLI mode
   python scr/orch.py "quantitative researchers Canada"

   # or Flask API mode
   flask run --app scr/app.py
   curl -X POST http://127.0.0.1:5000/run -H 'Content-Type: application/json' \
        -d '{"query":"AI ethics researchers"}'
   ```

5. **Verify results**

   ```sql
   SELECT full_name, summary, quality_score FROM person ORDER BY created_at DESC LIMIT 5;
   SELECT p.full_name, e.address FROM person p JOIN email e ON p.id=e.person_id;
   ```

---

## ğŸ”§ Tips & Next Steps

* **Stub summarizer** during dev: have PhaseÂ 3 return snippets to avoid VRAM issues.
* **Swap models** by editing `models/manifest.json` and rerunning `download_models.py`.
* **Scale**: containerize the `app` by providing a Dockerfile and uncommenting the `app:` service in `docker-compose.yml`.
* **Backups**: snapshot `/data/ldb/pgdata` regularly; your code sits on WD\_BLACK.
* **UI**: build a front-end or integrate into your book-editor agent under `ui/app.py`.

---

Casey Jussaume
